{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data # 获取迭代数据\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader,random_split\n",
    "from torch.autograd import Variable # 获取变量\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "clip = 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.load(\"../input_data/data.npy\")\n",
    "all_label = np.load(\"../input_data/label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.from_numpy(all_data)\n",
    "all_label = torch.from_numpy(all_label)\n",
    "all_data=all_data.float()\n",
    "all_label=all_label.long()\n",
    "dataset=TensorDataset(all_data,all_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和测试集\n",
    "ratio = 0.8\n",
    "seed = 30\n",
    "num_train = int(len(dataset) * ratio)\n",
    "num_test = len(dataset) - num_train\n",
    "    \n",
    "train_dataset, validate_dataset = random_split(dataset, [num_train, num_test],torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 1221\n",
      "validate_dataset: 306\n",
      "batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "#Load to DataLoader\n",
    "print(\"train_dataset:\",len(train_dataset))\n",
    "print(\"validate_dataset:\",len(validate_dataset))\n",
    "print(\"batch_size:\",batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validate_dataset,batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据包装\n",
    "batch_size = 4\n",
    "trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuda = True\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EagleC_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #卷积\n",
    "        self.features_ = nn.Sequential(nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "                                        ,nn.ReLU(inplace=True)\n",
    "                                       ,nn.MaxPool2d(2)\n",
    "                                       \n",
    "                                       ,nn.Conv2d(32,64,3,stride=1,padding=1)\n",
    "                                        ,nn.ReLU(inplace=True)\n",
    "                                       ,nn.MaxPool2d(2)\n",
    "                                      )\n",
    "        #分类\n",
    "        #根据net输出的形状确定\n",
    "        self.clf_ = nn.Sequential(nn.Dropout(0.5)\n",
    "                                  ,nn.Linear(64*7*7,512)\n",
    "                                  ,nn.ReLU(inplace=True)\n",
    "                                 ,nn.Linear(512,6)\n",
    "                                 ,nn.Sigmoid()\n",
    "                                 )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.features_(x) #用特征提取的架构提取特征\n",
    "        x = x.view(-1,64*7*7) #调整数据结构，拉平数据\n",
    "        output = self.clf_(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EagleC_CNN().to(device)\n",
    "#多分类\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, clip,criterion):\n",
    "\tmodel.train()\n",
    "\t#损失\n",
    "\tloss_sum = 0.0\n",
    "\tfor i, (data, target) in enumerate(train_loader):\n",
    "\t\tif i == (len(train_loader) - 1):\n",
    "\t\t\tcontinue\n",
    "\t\tdata, target = Variable(data).to(device), Variable(target, requires_grad=False).to(device)\n",
    "\t\t#将模型的参数梯度初始化为0\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t#得到结果\n",
    "\t\toutput = model(data)\n",
    "\t\t# loss function\n",
    "\t\tloss = criterion(output, target)\n",
    "\t\tloss.backward()\n",
    "\t\t#获取当前lr\n",
    "\t\tlr_current = get_lr(optimizer)\n",
    "\t\tclip2 = clip/lr_current\n",
    "\t\t#梯度裁剪\n",
    "\t\tnn.utils.clip_grad_norm_(model.parameters(),clip2)\n",
    "\t\toptimizer.step() #更新参数\n",
    "\t\tloss_sum = loss_sum + loss.item()\n",
    "\t#平均的损失函数\n",
    "\treturn loss_sum/i\n",
    "\t\n",
    "# validation\n",
    "def validate(model, device, validation_loader,criterion):\n",
    "\tmodel.eval()\n",
    "\tloss_sum = 0.0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (data, target) in enumerate(validation_loader):\n",
    "\t\t\tdata, target = Variable(data).to(device), Variable(target, requires_grad=False).to(device)\n",
    "\t\t\toutput = model(data)\n",
    "\t\t\t# loss function\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\t\t\t#计算损失函数\n",
    "\t\t\tloss_sum = loss_sum + loss.item()\n",
    "\t#平均的损失函数\n",
    "\treturn loss_sum/i\n",
    "\n",
    "# get current learning rate\n",
    "def get_lr(optimizer):\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\treturn param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,lr:0.01,loss train:1.552,loss validate:1.649\n",
      "epoch:2,lr:0.01,loss train:1.551,loss validate:1.626\n",
      "epoch:3,lr:0.001,loss train:1.552,loss validate:1.626\n",
      "epoch:4,lr:0.001,loss train:1.551,loss validate:1.649\n",
      "epoch:5,lr:0.001,loss train:1.553,loss validate:1.626\n",
      "epoch:6,lr:0.001,loss train:1.552,loss validate:1.672\n",
      "epoch:7,lr:0.001,loss train:1.55,loss validate:1.649\n",
      "epoch:8,lr:0.001,loss train:1.551,loss validate:1.626\n",
      "epoch:9,lr:0.001,loss train:1.552,loss validate:1.626\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "for epoch in range(1, epochs):\n",
    "    loss_train = train(model, device, train_loader, optimizer,clip,criterion) \n",
    "    loss_validate = validate(model, device, validation_loader,criterion)\n",
    "    scheduler.step(loss_validate)\t\n",
    "    lr_current = get_lr(optimizer)\n",
    "    print(\"epoch:{},lr:{},loss train:{},loss validate:{}\".format(epoch, lr_current, np.round(loss_train,3), np.round(loss_validate,3)) ) \n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), \"model_epoch\" + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "PATH = 'model.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load()\n",
    "target_test = np.load()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(data.TensorDataset(torch.from_numpy(data_test), torch.from_numpy(np.zeros(target_test.shape[0]))), batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:要改\n",
    "result = np.zeros((low_res_test.shape[0],1,28,28))\n",
    "for i, (data, _) in enumerate(test_loader):\n",
    "\tdata2 = Variable(data).to(device)\n",
    "\toutput = Net(data2)\n",
    "\tresulti = output.cpu().data.numpy()\n",
    "\tresulti = np.squeeze(resulti)\n",
    "\ti1 = i * args.batch_size\n",
    "\ti2 = i1 + args.batch_size\n",
    "\tif i == int(low_res_test.shape[0]/args.batch_size):\n",
    "\t\ti2 = low_res_test.shape[0]\n",
    "\tresult[i1:i2,0,:,:] = resulti\n",
    "\n",
    "np.save(args.file_test_predicted, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c2b4c422c51196e5a13ef0382ae654019ed0bc9e75f046108fddc534a620d08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
