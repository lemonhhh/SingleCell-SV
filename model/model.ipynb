{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data # 获取迭代数据\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader,random_split\n",
    "from torch.autograd import Variable # 获取变量\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchensemble import VotingClassifier,FusionClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "clip = 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.load(\"../input_data/data.npy\")\n",
    "all_label = np.load(\"../input_data/label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.from_numpy(all_data)\n",
    "all_label = torch.from_numpy(all_label)\n",
    "all_data=all_data.float()\n",
    "all_label=all_label.long()\n",
    "#pack to dataset\n",
    "dataset=TensorDataset(all_data,all_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和测试集\n",
    "\n",
    "seed = 30\n",
    "num_train_all = int(len(dataset) * 0.9)\n",
    "num_test = len(dataset) - num_train_all\n",
    "num_train = int(num_train_all * 0.8)\n",
    "num_val = num_train_all - num_train\n",
    "\n",
    "train_dataset, validate_dataset, test_dataset = random_split(dataset, [num_train, num_val,num_test],torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 11875\n",
      "validate_dataset: 2969\n",
      "test_dataset: 1650\n",
      "batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "#Load to DataLoader\n",
    "print(\"train_dataset:\",len(train_dataset))\n",
    "print(\"validate_dataset:\",len(validate_dataset))\n",
    "print(\"test_dataset:\",len(test_dataset))\n",
    "print(\"batch_size:\",batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validate_dataset,batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuda = True\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EagleC_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #卷积\n",
    "        self.features_ = nn.Sequential(nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "                                        ,nn.ReLU(inplace=True)\n",
    "                                       ,nn.MaxPool2d(2)\n",
    "                                       \n",
    "                                       ,nn.Conv2d(32,64,3,stride=1,padding=1)\n",
    "                                        ,nn.ReLU(inplace=True)\n",
    "                                       ,nn.MaxPool2d(2)\n",
    "                                      )\n",
    "        #分类\n",
    "        #根据net输出的形状确定\n",
    "        self.clf_ = nn.Sequential(nn.Dropout(0.5)\n",
    "                                  ,nn.Linear(64*5*5,512)\n",
    "                                  ,nn.ReLU(inplace=True)\n",
    "                                 ,nn.Linear(512,6)\n",
    "                                 ,nn.Sigmoid()\n",
    "                                 )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.features_(x) #用特征提取的架构提取特征\n",
    "        x = x.view(-1,64*5*5) #调整数据结构，拉平数据\n",
    "        output = self.clf_(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "                                        ,nn.ReLU(inplace=True)\n",
    "                                       ,nn.MaxPool2d(2)\n",
    "                                       \n",
    "                                       ,nn.Conv2d(32,64,3,stride=1,padding=1)\n",
    "                                        ,nn.ReLU(inplace=True)\n",
    "                                       ,nn.MaxPool2d(2)\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(1,1,21,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 5, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VotingClassifier(\n",
    "    estimator=EagleC_CNN,\n",
    "    n_estimators=5,\n",
    "    cuda=False,\n",
    ")\n",
    "\n",
    "# model = EagleC_CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#多分类\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.set_criterion(criterion)\n",
    "#优化器\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.set_optimizer('Adam',             # parameter optimizer\n",
    "                    lr=1e-3,            # learning rate of the optimizer\n",
    "                    weight_decay=5e-4)  # weight decay of the optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 1.79046 | Correct: 3/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 1.40906 | Correct: 8/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 1.52825 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 1.47766 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 1.54604 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 1.40570 | Correct: 10/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 1.53066 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 1.43931 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 000 | Loss: 1.79627 | Correct: 0/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 100 | Loss: 1.57077 | Correct: 2/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 200 | Loss: 1.56900 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 300 | Loss: 1.46730 | Correct: 4/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 400 | Loss: 1.47705 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 500 | Loss: 1.46293 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 600 | Loss: 1.60314 | Correct: 2/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 700 | Loss: 1.48053 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 000 | Loss: 1.79046 | Correct: 3/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 100 | Loss: 1.60876 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 200 | Loss: 1.46551 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 300 | Loss: 1.41027 | Correct: 8/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 400 | Loss: 1.49681 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 500 | Loss: 1.54409 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 600 | Loss: 1.52531 | Correct: 8/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 700 | Loss: 1.55861 | Correct: 4/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 000 | Loss: 1.79148 | Correct: 1/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 100 | Loss: 1.80638 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 200 | Loss: 1.60589 | Correct: 1/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 300 | Loss: 1.54723 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 400 | Loss: 1.55710 | Correct: 3/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 500 | Loss: 1.51044 | Correct: 9/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 600 | Loss: 1.49666 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 700 | Loss: 1.51540 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 000 | Loss: 1.79036 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 100 | Loss: 1.44733 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 200 | Loss: 1.54644 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 300 | Loss: 1.42724 | Correct: 9/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 400 | Loss: 1.54702 | Correct: 4/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 500 | Loss: 1.42048 | Correct: 9/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 600 | Loss: 1.50189 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 700 | Loss: 1.57520 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 000 | Loss: 1.45332 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 100 | Loss: 1.49615 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 200 | Loss: 1.52307 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 300 | Loss: 1.44083 | Correct: 8/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 400 | Loss: 1.42425 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 500 | Loss: 1.46562 | Correct: 9/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 600 | Loss: 1.56686 | Correct: 3/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 700 | Loss: 1.52227 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 000 | Loss: 1.43210 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 100 | Loss: 1.47875 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 200 | Loss: 1.40228 | Correct: 4/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 300 | Loss: 1.49383 | Correct: 8/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 400 | Loss: 1.49450 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 500 | Loss: 1.53416 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 600 | Loss: 1.55708 | Correct: 3/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 700 | Loss: 1.48896 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 000 | Loss: 1.52442 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 100 | Loss: 1.50786 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 200 | Loss: 1.45203 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 300 | Loss: 1.62869 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 400 | Loss: 1.59016 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 500 | Loss: 1.52508 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 600 | Loss: 1.42675 | Correct: 8/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 700 | Loss: 1.42972 | Correct: 9/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 000 | Loss: 1.53342 | Correct: 2/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 100 | Loss: 1.58205 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 200 | Loss: 1.45712 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 300 | Loss: 1.53311 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 400 | Loss: 1.44213 | Correct: 8/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 500 | Loss: 1.65047 | Correct: 3/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 600 | Loss: 1.51525 | Correct: 9/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 700 | Loss: 1.62070 | Correct: 4/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 000 | Loss: 1.67042 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 100 | Loss: 1.43503 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 200 | Loss: 1.45057 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 300 | Loss: 1.39301 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 400 | Loss: 1.51413 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 500 | Loss: 1.40944 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 600 | Loss: 1.38595 | Correct: 9/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 700 | Loss: 1.33989 | Correct: 8/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 000 | Loss: 1.52462 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 100 | Loss: 1.43542 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 200 | Loss: 1.49549 | Correct: 3/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 300 | Loss: 1.59263 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 400 | Loss: 1.43828 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 500 | Loss: 1.39230 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 600 | Loss: 1.58036 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 700 | Loss: 1.43195 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 000 | Loss: 1.42791 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 100 | Loss: 1.52977 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 200 | Loss: 1.42269 | Correct: 11/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 300 | Loss: 1.41133 | Correct: 8/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 400 | Loss: 1.46666 | Correct: 10/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 500 | Loss: 1.55161 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 600 | Loss: 1.43043 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 700 | Loss: 1.54256 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 000 | Loss: 1.56613 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 100 | Loss: 1.48544 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 200 | Loss: 1.42790 | Correct: 10/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 300 | Loss: 1.50770 | Correct: 8/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 400 | Loss: 1.39320 | Correct: 11/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 500 | Loss: 1.46323 | Correct: 8/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 600 | Loss: 1.52232 | Correct: 3/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 700 | Loss: 1.42150 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 000 | Loss: 1.55847 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 100 | Loss: 1.46036 | Correct: 4/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 200 | Loss: 1.43743 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 300 | Loss: 1.56584 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 400 | Loss: 1.55939 | Correct: 4/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 500 | Loss: 1.46052 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 600 | Loss: 1.40615 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 700 | Loss: 1.45770 | Correct: 9/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 000 | Loss: 1.45106 | Correct: 2/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 100 | Loss: 1.42084 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 200 | Loss: 1.50287 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 300 | Loss: 1.40635 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 400 | Loss: 1.52040 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 500 | Loss: 1.42789 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 600 | Loss: 1.53116 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 700 | Loss: 1.48840 | Correct: 9/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 000 | Loss: 1.44762 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 100 | Loss: 1.53577 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 200 | Loss: 1.56792 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 300 | Loss: 1.45239 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 400 | Loss: 1.50131 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 500 | Loss: 1.46656 | Correct: 1/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 600 | Loss: 1.56899 | Correct: 8/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 700 | Loss: 1.63006 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 000 | Loss: 1.50583 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 100 | Loss: 1.48083 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 200 | Loss: 1.36312 | Correct: 10/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 300 | Loss: 1.62026 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 400 | Loss: 1.39706 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 500 | Loss: 1.48825 | Correct: 8/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 600 | Loss: 1.48338 | Correct: 11/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 700 | Loss: 1.49674 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 000 | Loss: 1.38834 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 100 | Loss: 1.54760 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 200 | Loss: 1.53054 | Correct: 8/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 300 | Loss: 1.44173 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 400 | Loss: 1.58775 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 500 | Loss: 1.32626 | Correct: 10/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 600 | Loss: 1.53693 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 700 | Loss: 1.58508 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 000 | Loss: 1.54063 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 100 | Loss: 1.44863 | Correct: 8/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 200 | Loss: 1.47944 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 300 | Loss: 1.46725 | Correct: 8/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 400 | Loss: 1.41186 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 500 | Loss: 1.51704 | Correct: 4/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 600 | Loss: 1.44017 | Correct: 11/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 700 | Loss: 1.43388 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 000 | Loss: 1.51562 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 100 | Loss: 1.56470 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 200 | Loss: 1.46088 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 300 | Loss: 1.35340 | Correct: 10/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 400 | Loss: 1.43133 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 500 | Loss: 1.59042 | Correct: 4/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 600 | Loss: 1.47294 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 700 | Loss: 1.45252 | Correct: 5/16\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_loader=train_loader,  # training data\n",
    "          epochs=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_loader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, clip,criterion):\n",
    "\tmodel.train()\n",
    "\t#损失\n",
    "\tloss_sum = 0.0\n",
    "\tfor i, (data, target) in enumerate(train_loader):\n",
    "\t\tif i == (len(train_loader) - 1):\n",
    "\t\t\tcontinue\n",
    "\t\tdata, target = Variable(data).to(device), Variable(target, requires_grad=False).to(device)\n",
    "\t\t#将模型的参数梯度初始化为0\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t#得到结果\n",
    "\t\toutput = model(data)\n",
    "\t\t# loss function\n",
    "\t\tloss = criterion(output, target)\n",
    "\t\tloss.backward()\n",
    "\t\t#获取当前lr\n",
    "\t\tlr_current = get_lr(optimizer)\n",
    "\t\tclip2 = clip/lr_current\n",
    "\t\t#梯度裁剪\n",
    "\t\tnn.utils.clip_grad_norm_(model.parameters(),clip2)\n",
    "\t\toptimizer.step() #更新参数\n",
    "\t\tloss_sum = loss_sum + loss.item()\n",
    "\t#平均的损失函数\n",
    "\treturn loss_sum/i\n",
    "\t\n",
    "# validation\n",
    "def validate(model, device, validation_loader,criterion):\n",
    "\tmodel.eval()\n",
    "\tloss_sum = 0.0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (data, target) in enumerate(validation_loader):\n",
    "\t\t\tdata, target = Variable(data).to(device), Variable(target, requires_grad=False).to(device)\n",
    "\t\t\toutput = model(data)\n",
    "\t\t\t# loss function\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\t\t\t#计算损失函数\n",
    "\t\t\tloss_sum = loss_sum + loss.item()\n",
    "\t#平均的损失函数\n",
    "\treturn loss_sum/i\n",
    "\n",
    "# get current learning rate\n",
    "def get_lr(optimizer):\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\treturn param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#训练\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs):\n\u001b[0;32m----> 3\u001b[0m     loss_train \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer,clip,criterion) \n\u001b[1;32m      4\u001b[0m     loss_validate \u001b[39m=\u001b[39m validate(model, device, validation_loader,criterion)\n\u001b[1;32m      5\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(loss_validate)\t\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "for epoch in range(1, epochs):\n",
    "    loss_train = train(model, device, train_loader, optimizer,clip,criterion) \n",
    "    loss_validate = validate(model, device, validation_loader,criterion)\n",
    "    scheduler.step(loss_validate)\t\n",
    "    lr_current = get_lr(optimizer)\n",
    "    print(\"epoch:{},lr:{},loss train:{},loss validate:{}\".format(epoch, lr_current, np.round(loss_train,3), np.round(loss_validate,3)) ) \n",
    "    # save the model\n",
    "    # torch.save(model.state_dict(), \"model_epoch\" + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "PATH = 'model.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The type of input X should be one of {{torch.Tensor, np.ndarray}}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(test_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/sv/lib/python3.8/site-packages/torchensemble/voting.py:272\u001b[0m, in \u001b[0;36mVotingClassifier.predict\u001b[0;34m(self, *x)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m@torchensemble_model_doc\u001b[39m(item\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mx):\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\u001b[39m*\u001b[39;49mx)\n",
      "File \u001b[0;32m~/anaconda3/envs/sv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sv/lib/python3.8/site-packages/torchensemble/_base.py:196\u001b[0m, in \u001b[0;36mBaseModule.predict\u001b[0;34m(self, *x)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    193\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe type of input X should be one of \u001b[39m\u001b[39m{{\u001b[39m\u001b[39mtorch.Tensor,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m np.ndarray}}.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         )\n\u001b[0;32m--> 196\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    198\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39mx_device)\n\u001b[1;32m    199\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[0;31mValueError\u001b[0m: The type of input X should be one of {{torch.Tensor, np.ndarray}}."
     ]
    }
   ],
   "source": [
    "accuracy = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EagleC_CNN()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test: 35 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        # 人为最大的就是分到的类\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on test: {100 * correct // total} %')\n",
    "#还是比随机结果要好一些的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c2b4c422c51196e5a13ef0382ae654019ed0bc9e75f046108fddc534a620d08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
