{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data  # 获取迭代数据\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "from torch.autograd import Variable  # 获取变量\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchensemble import VotingClassifier, FusionClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#setting\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "clip = 0.01\n",
    "\n",
    "no_cuda = False\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_data = np.load(\"../input_data/data.npy\")\n",
    "all_label = np.load(\"../input_data/label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_data = torch.from_numpy(all_data).to(device)\n",
    "all_label = torch.from_numpy(all_label).to(device)\n",
    "all_data = all_data.float()\n",
    "all_label = all_label.long()\n",
    "#pack to dataset\n",
    "dataset = TensorDataset(all_data, all_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#划分训练集和测试集\n",
    "\n",
    "seed = 30\n",
    "num_train_all = int(len(dataset) * 0.9)\n",
    "num_test = len(dataset) - num_train_all\n",
    "num_train = int(num_train_all * 0.8)\n",
    "num_val = num_train_all - num_train\n",
    "\n",
    "train_dataset, validate_dataset, test_dataset = random_split(dataset, [num_train, num_val, num_test],\n",
    "                                                             torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 11875\n",
      "validate_dataset: 2969\n",
      "test_dataset: 1650\n",
      "batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "#Load to DataLoader\n",
    "print(\"train_dataset:\", len(train_dataset))\n",
    "print(\"validate_dataset:\", len(validate_dataset))\n",
    "print(\"test_dataset:\", len(test_dataset))\n",
    "print(\"batch_size:\", batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validate_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EagleC_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #卷积\n",
    "        self.features_ = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "                                       , nn.ReLU(inplace=True)\n",
    "                                       , nn.MaxPool2d(2)\n",
    "\n",
    "                                       , nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "                                       , nn.ReLU(inplace=True)\n",
    "                                       , nn.MaxPool2d(2)\n",
    "                                       )\n",
    "        #分类\n",
    "        #根据net输出的形状确定\n",
    "        self.clf_ = nn.Sequential(nn.Dropout(0.5)\n",
    "                                  , nn.Linear(64 * 5 * 5, 512)\n",
    "                                  , nn.ReLU(inplace=True)\n",
    "                                  , nn.Linear(512, 6)\n",
    "                                  , nn.Sigmoid()\n",
    "                                  )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_(x)  #用特征提取的架构提取特征\n",
    "        x = x.view(-1, 64 * 5 * 5)  #调整数据结构，拉平数据\n",
    "        output = self.clf_(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = VotingClassifier(\n",
    "    estimator=EagleC_CNN,\n",
    "    n_estimators=5,\n",
    "    cuda=False,\n",
    ").to(device)\n",
    "\n",
    "# model = EagleC_CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#多分类\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.set_criterion(criterion)\n",
    "#优化器\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.set_optimizer('Adam',  # parameter optimizer\n",
    "                    lr=1e-3,  # learning rate of the optimizer\n",
    "                    weight_decay=5e-4)  # weight decay of the optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 1.79369 | Correct: 2/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 1.61582 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 1.52333 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 1.55128 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 1.47394 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 1.53665 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 1.48053 | Correct: 9/16\n",
      "Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 1.73456 | Correct: 3/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 000 | Loss: 1.78374 | Correct: 9/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 100 | Loss: 1.59709 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 200 | Loss: 1.40919 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 300 | Loss: 1.52196 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 400 | Loss: 1.45946 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 500 | Loss: 1.55912 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 600 | Loss: 1.59816 | Correct: 4/16\n",
      "Estimator: 001 | Epoch: 000 | Batch: 700 | Loss: 1.61039 | Correct: 2/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 000 | Loss: 1.78623 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 100 | Loss: 1.50337 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 200 | Loss: 1.58015 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 300 | Loss: 1.49916 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 400 | Loss: 1.57449 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 500 | Loss: 1.63657 | Correct: 9/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 600 | Loss: 1.39209 | Correct: 8/16\n",
      "Estimator: 002 | Epoch: 000 | Batch: 700 | Loss: 1.41857 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 000 | Loss: 1.79232 | Correct: 1/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 100 | Loss: 1.68204 | Correct: 3/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 200 | Loss: 1.73577 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 300 | Loss: 1.63773 | Correct: 9/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 400 | Loss: 1.45421 | Correct: 8/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 500 | Loss: 1.54094 | Correct: 8/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 600 | Loss: 1.48516 | Correct: 4/16\n",
      "Estimator: 003 | Epoch: 000 | Batch: 700 | Loss: 1.45069 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 000 | Loss: 1.79160 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 100 | Loss: 1.66948 | Correct: 4/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 200 | Loss: 1.50352 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 300 | Loss: 1.48234 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 400 | Loss: 1.48825 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 500 | Loss: 1.47533 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 600 | Loss: 1.51761 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 000 | Batch: 700 | Loss: 1.51658 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 000 | Loss: 1.55080 | Correct: 3/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 100 | Loss: 1.47645 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 200 | Loss: 1.49989 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 300 | Loss: 1.56647 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 400 | Loss: 1.43054 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 500 | Loss: 1.55961 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 600 | Loss: 1.53264 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 001 | Batch: 700 | Loss: 1.54842 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 000 | Loss: 1.48670 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 100 | Loss: 1.49675 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 200 | Loss: 1.45323 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 300 | Loss: 1.54578 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 400 | Loss: 1.53262 | Correct: 3/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 500 | Loss: 1.36699 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 600 | Loss: 1.47487 | Correct: 10/16\n",
      "Estimator: 001 | Epoch: 001 | Batch: 700 | Loss: 1.51394 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 000 | Loss: 1.50044 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 100 | Loss: 1.49844 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 200 | Loss: 1.48799 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 300 | Loss: 1.36242 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 400 | Loss: 1.35826 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 500 | Loss: 1.51095 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 600 | Loss: 1.42230 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 001 | Batch: 700 | Loss: 1.51314 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 000 | Loss: 1.52713 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 100 | Loss: 1.47508 | Correct: 3/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 200 | Loss: 1.57537 | Correct: 4/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 300 | Loss: 1.58854 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 400 | Loss: 1.55251 | Correct: 4/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 500 | Loss: 1.44499 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 600 | Loss: 1.38577 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 001 | Batch: 700 | Loss: 1.49140 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 000 | Loss: 1.53284 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 100 | Loss: 1.59439 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 200 | Loss: 1.52438 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 300 | Loss: 1.35781 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 400 | Loss: 1.48495 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 500 | Loss: 1.47498 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 600 | Loss: 1.48186 | Correct: 5/16\n",
      "Estimator: 004 | Epoch: 001 | Batch: 700 | Loss: 1.43520 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 000 | Loss: 1.38325 | Correct: 9/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 100 | Loss: 1.57348 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 200 | Loss: 1.47193 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 300 | Loss: 1.52288 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 400 | Loss: 1.48100 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 500 | Loss: 1.38628 | Correct: 9/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 600 | Loss: 1.38347 | Correct: 7/16\n",
      "Estimator: 000 | Epoch: 002 | Batch: 700 | Loss: 1.54540 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 000 | Loss: 1.42252 | Correct: 9/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 100 | Loss: 1.52150 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 200 | Loss: 1.49276 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 300 | Loss: 1.43087 | Correct: 4/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 400 | Loss: 1.43642 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 500 | Loss: 1.50781 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 600 | Loss: 1.46810 | Correct: 9/16\n",
      "Estimator: 001 | Epoch: 002 | Batch: 700 | Loss: 1.54037 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 000 | Loss: 1.54041 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 100 | Loss: 1.51982 | Correct: 4/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 200 | Loss: 1.54261 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 300 | Loss: 1.58010 | Correct: 3/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 400 | Loss: 1.49357 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 500 | Loss: 1.49584 | Correct: 5/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 600 | Loss: 1.37647 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 002 | Batch: 700 | Loss: 1.57074 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 000 | Loss: 1.51348 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 100 | Loss: 1.41692 | Correct: 10/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 200 | Loss: 1.44284 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 300 | Loss: 1.53610 | Correct: 6/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 400 | Loss: 1.57869 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 500 | Loss: 1.46224 | Correct: 8/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 600 | Loss: 1.48093 | Correct: 8/16\n",
      "Estimator: 003 | Epoch: 002 | Batch: 700 | Loss: 1.48712 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 000 | Loss: 1.56842 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 100 | Loss: 1.56685 | Correct: 6/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 200 | Loss: 1.45273 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 300 | Loss: 1.48163 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 400 | Loss: 1.64382 | Correct: 2/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 500 | Loss: 1.57572 | Correct: 2/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 600 | Loss: 1.54064 | Correct: 9/16\n",
      "Estimator: 004 | Epoch: 002 | Batch: 700 | Loss: 1.57989 | Correct: 3/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 000 | Loss: 1.36724 | Correct: 6/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 100 | Loss: 1.55661 | Correct: 9/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 200 | Loss: 1.48432 | Correct: 8/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 300 | Loss: 1.54087 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 400 | Loss: 1.61307 | Correct: 4/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 500 | Loss: 1.54255 | Correct: 3/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 600 | Loss: 1.51742 | Correct: 5/16\n",
      "Estimator: 000 | Epoch: 003 | Batch: 700 | Loss: 1.53368 | Correct: 8/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 000 | Loss: 1.54665 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 100 | Loss: 1.37917 | Correct: 8/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 200 | Loss: 1.60061 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 300 | Loss: 1.56079 | Correct: 4/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 400 | Loss: 1.44512 | Correct: 6/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 500 | Loss: 1.49946 | Correct: 7/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 600 | Loss: 1.44823 | Correct: 5/16\n",
      "Estimator: 001 | Epoch: 003 | Batch: 700 | Loss: 1.48901 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 000 | Loss: 1.55190 | Correct: 3/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 100 | Loss: 1.40362 | Correct: 7/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 200 | Loss: 1.43580 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 300 | Loss: 1.51551 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 400 | Loss: 1.54198 | Correct: 3/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 500 | Loss: 1.52798 | Correct: 3/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 600 | Loss: 1.47027 | Correct: 6/16\n",
      "Estimator: 002 | Epoch: 003 | Batch: 700 | Loss: 1.35471 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 000 | Loss: 1.45796 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 100 | Loss: 1.42297 | Correct: 8/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 200 | Loss: 1.52244 | Correct: 9/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 300 | Loss: 1.49211 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 400 | Loss: 1.38881 | Correct: 7/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 500 | Loss: 1.55506 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 600 | Loss: 1.56073 | Correct: 5/16\n",
      "Estimator: 003 | Epoch: 003 | Batch: 700 | Loss: 1.48605 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 000 | Loss: 1.44993 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 100 | Loss: 1.39770 | Correct: 9/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 200 | Loss: 1.49111 | Correct: 8/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 300 | Loss: 1.50330 | Correct: 7/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 400 | Loss: 1.44139 | Correct: 9/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 500 | Loss: 1.53548 | Correct: 9/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 600 | Loss: 1.49366 | Correct: 3/16\n",
      "Estimator: 004 | Epoch: 003 | Batch: 700 | Loss: 1.52317 | Correct: 8/16\n",
      "Use 196.01547193527222 s\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "model.fit(train_loader=train_loader,  # training data\n",
    "          epochs=4)\n",
    "print(\"Use {} s\".format(time.time() - time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The type of input X should be one of {{torch.Tensor, np.ndarray}}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(test_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/sv/lib/python3.8/site-packages/torchensemble/voting.py:272\u001b[0m, in \u001b[0;36mVotingClassifier.predict\u001b[0;34m(self, *x)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m@torchensemble_model_doc\u001b[39m(item\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mx):\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\u001b[39m*\u001b[39;49mx)\n",
      "File \u001b[0;32m~/anaconda3/envs/sv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sv/lib/python3.8/site-packages/torchensemble/_base.py:196\u001b[0m, in \u001b[0;36mBaseModule.predict\u001b[0;34m(self, *x)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    193\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe type of input X should be one of \u001b[39m\u001b[39m{{\u001b[39m\u001b[39mtorch.Tensor,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m np.ndarray}}.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         )\n\u001b[0;32m--> 196\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    198\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39mx_device)\n\u001b[1;32m    199\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[0;31mValueError\u001b[0m: The type of input X should be one of {{torch.Tensor, np.ndarray}}."
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c2b4c422c51196e5a13ef0382ae654019ed0bc9e75f046108fddc534a620d08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
